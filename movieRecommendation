# Project 3: Movie Recommendation System Using Embeddings

Welcome to the third project in our course, where we will design and build a Movie Recommendation System using the power of OpenAI embeddings. In this module, we'll tap into the advanced natural language understanding capabilities of OpenAI's machine learning models to analyze and match movie titles and descriptions, providing personalized recommendations to users.

## What You Will Learn

- **Understanding Embeddings**: Grasp the concept of embeddings and how they represent textual data in a way that captures semantic meaning.
- **OpenAI's Embeddings API**: Get to know OpenAI's API for generating embeddings and learn how to leverage it to transform movie metadata into a suitable format for recommendation algorithms.
- **Recommendation Logic**: Implement the logic for a recommendation system that uses cosine similarity to find the most relevant movie suggestions based on user preferences.
- **Pinecone**: Working with Pinecone Vectore Database

## Project Objectives

This project will guide you to build a recommendation system that:

1. **Processes Movie Data**: Converts movie titles and descriptions into embeddings that capture the essence of the content.
2. **Calculates Similarities**: Uses the embeddings to find similarities between movies based on user queries or past user interactions.
3. **Generates Recommendations**: Offers a list of movie recommendations tailored to the user's tastes and viewing history.
4. **Pinecone**: Working with Pinecone Vectore Database


## Prerequisites

Ensure you're ready to start with:

- A Google Colab account set up and ready to go.
- Fundamental knowledge of Python and machine learning concepts.
- Access to OpenAI API for embeddings (obtain your API key from [OpenAI](https://platform.openai.com/account/api-keys)).

## Let's Begin

Are you prepared to enter the world of personalized content recommendation? Let's harness the capabilities of embeddings and create a Movie Recommendation System!


# 2. Libraries import

!pip install openai

import os
import openai
import numpy as np
import pandas as pd

from openai import OpenAI

# 3. Sending a first request to OpenAI API


### 3.1 Setting up API Key

os.environ["OPENAI_API_KEY"] = "sk-proj-k5KA4O6qMdRvQSVavu6GT3BlbkFJRDbE95iQ6p1rFeQKmwmL"
client = OpenAI()

### 3.2 Vectors and their similarity


### Embeddings:

Imagine you have a bunch of different fruits, and you want to describe each one on a piece of paper so that someone can understand what each fruit is like without seeing it. You’d write down things like the color, shape, size, and taste of each fruit. In the world of computers and AI, embeddings do something similar for words or movies.

An embedding is a way of turning words, sentences, or things like movies into a list of numbers (we call this list a "vector") that represents different features, just like the list you made for the fruits. For example, for movies, the numbers might represent how action-packed they are, whether they are romantic, if they are funny, and so on. These numbers aren't random; they are calculated so that movies with similar numbers have similar features.

![](https://cdn.sanity.io/images/vr8gru94/production/e016bbd4d7d57ff27e261adf1e254d2d3c609aac-2447x849.png)
Source: https://www.pinecone.io/learn/vector-embeddings/

### Vector Similarity:

Now, let’s say you have two lists of numbers for two different movies. How can you tell if the movies are similar? This is where vector similarity comes in.

Imagine you and a friend each have a toy car, and you race them side by side to see which one is faster. If the cars finish the race at almost the same time, you’d say they’re pretty similar in speed. Vector similarity does the same thing with the lists of numbers for the movies.

Computers use a method to "race" the vectors against each other, often using something called "cosine similarity." They check how close the numbers are in both lists. If the numbers are really close across both lists, it’s like two cars finishing at the same time, which means the movies are similar. If the numbers are far apart, then the movies are quite different, just like if one car finishes way ahead of the other.

So, in simple terms:

- **Embeddings** are like writing a detailed description of something (like a movie) in a special code of numbers that a computer can understand.
- **Vector similarity** is like a race to see how similar two sets of numbers (or embeddings) are, which tells us how similar the things they represent (like two movies) might be to each other.


![](https://cdn.sanity.io/images/vr8gru94/production/5a5ba7e0971f7b6dc4697732fa8adc59a46b6d8d-338x357.png)

Source: https://www.pinecone.io/learn/vector-similarity/

experiment_sentence = "The Terminator is a movie about AI going after human"

res = client.embeddings.create (
    model = "text-embedding-ada-002",
    input = experiment_sentence
)

len(res.data[0].embedding[:10])

## Similarity

toy_dataset = [
    "The Terminator is a movie that has AI-based robots inside of them",
    "Harry Potter is all amobut wizards and magic",
    "In the movie Matrix, AI already has become the most powerfull 'being'"
]

toy_embeddings = client.embeddings.create(
    model = "text-embedding-ada-002",
    input = toy_dataset
)

clean_embeds = []
for embed in toy_embeddings.data:
  clean_embeds.append(embed.embedding)

clean_embeds[0][:10]

user_request = input("Enter movie description: ")
user_vector = client.embeddings.create(
    model = "text-embedding-ada-002",
    input = toy_dataset
)

user_vector = user_vector.data[0].embedding

from scipy.spatial.distance import cosine, cdist

similarities = 1 - cdist(np.array(user_vector).reshape(1,-1), np.array(clean_embeds), "cosine")

## Recommending most similar vector

np.argsort(- similarities)

p_movies = [toy_dataset[id] for id in np.argsort(-similarities[0])]



# 4. Scaling to the big dataset

You can download dataset from here: https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset?select=movies_metadata.csv

data = pd.read_csv("movies_metadata.csv")

data.head()

subset = data[["original_title", "overview"]]

subset.head()

# Using only 100 movies for recommendation system to peresven money for API :)
small_dataset = subset.iloc[:100]
small_dataset.shape

# Drop missing values
small_dataset.dropna(inplace=True)
small_dataset.shape

movie_embeddings = client.embeddings.create(
    model = "text-embedding-ada-002",
    input = small_dataset['overview'].values.tolist(),
)

clean_movie_embeddings = []
for embed in movie_embeddings.data:
  clean_movie_embeddings.append(embed.embedding)

clean_movie_embeddings = np.arrays(clean_movie_embeddings)

user_request = input("Enter moive description: ")
user_vector = client.embeddings.create(
    model = "text-embedding-ada-002",
    input = user_request
)

user_vector = np.array(user_vector).reshape(1,-1)

scores = np.argsort(- cdist(user_vector, clean_movie_embeddings, "cosine"))

for i in scores[:10]:
  print(small_dataset.iloc[33]['original_title'])

### 5. Building movie recommender with Pinecone


Pinecone website: https://www.pinecone.io/

pip install pinecone-client

from pinecone import Pinecone

pc = Pinecone(api_key="4725851f-399d-413c-a8be-a07afa916a50")
index = pc.Index("movies")

for i in range(len(small_dataset)):
  upload_stats = index.upsert(
      vectors = [
          (
              str(i),
              clean_movie_embeddings[0].tolist(),
              { "title": small_dataset.iloc[i]['original_title' ]}
          )
      ]
  )

## Searching the most similar movie

user_request = input("Enter a movie description: ")
user_vector = client.embeddings.create(
    model = "text-embedding-ada-002",
    input = user_request
)
user_vector = user_vector.data[0].embedding

matches = index.query(
    user_vector,
    top_k = 10,
    include_metadata = True
)
